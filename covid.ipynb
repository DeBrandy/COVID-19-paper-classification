{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64c7231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e0877a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 250\n",
    "BATCH_SIZE = 4\n",
    "config_path = 'uncased_L-12_H-512_A-8/bert_config.json'\n",
    "checkpoint_path = 'uncased_L-12_H-512_A-8/bert_model.ckpt'\n",
    "dict_path = 'uncased_L-12_H-512_A-8/vocab.txt'\n",
    "#config_path = 'biobert_v1.1_pubmed/bert_config.json'\n",
    "#checkpoint_path = 'biobert_v1.1_pubmed/model.ckpt'\n",
    "#dict_path = 'biobert_v1.1_pubmed/vocab.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c1c0e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "with codecs.open(dict_path, 'r', 'utf-8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "\n",
    "#分词\n",
    "class OurTokenizer(Tokenizer):\n",
    "    def _tokenize(self, text):\n",
    "        R = []\n",
    "        for c in text:\n",
    "            if c in self._token_dict:\n",
    "                R.append(c)\n",
    "            else:\n",
    "                R.append('[UNK]')   # 剩余的字符是[UNK]\n",
    "        return R\n",
    "\n",
    "\n",
    "tokenizer = OurTokenizer(token_dict)\n",
    "\n",
    "#填充成一样长\n",
    "def seq_padding(X, padding=0):\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    return np.array([\n",
    "        np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "999f7e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin data processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wyq/anaconda3/envs/env_python3.6/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish data processing!\n"
     ]
    }
   ],
   "source": [
    "print(\"begin data processing...\")\n",
    "train_df = pd.read_csv(\"data/train.csv\").fillna(value=\"\")\n",
    "valid_df = pd.read_csv(\"data/valid.csv\").fillna(value=\"\")\n",
    "test_df = pd.read_csv(\"data/test.csv\").fillna(value=\"\")\n",
    "#删除label为空的行\n",
    "train_df.drop(train_df[train_df['label']==''].index)\n",
    "valid_df.drop(valid_df[valid_df['label']==''].index)\n",
    "test_df.drop(valid_df[test_df['label']==''].index)\n",
    "\n",
    "select_labels = train_df[\"label\"].unique()\n",
    "labels = []\n",
    "#把8个label都提取出来，dict：0:treatment,1:...\n",
    "for label in select_labels:\n",
    "    if \";\" not in label:\n",
    "        if label not in labels and label:\n",
    "            labels.append(label)\n",
    "\n",
    "# with open(\"label.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(json.dumps(dict(zip(range(len(labels)), labels)), ensure_ascii=False, indent=2))\n",
    "\n",
    "train_data = []\n",
    "valid_data = []\n",
    "test_data = []\n",
    "for i in range(train_df.shape[0]):\n",
    "    pmid, journal, title, abstract, keywords, label,pub_type, authors, date1, doi, date2, label_category = train_df.iloc[i, :]\n",
    "    label_id = [0] * len(labels)\n",
    "    for j, _ in enumerate(labels):\n",
    "        for separate_label in label.split(\";\"):\n",
    "            if _ == separate_label:\n",
    "                label_id[j] = 1\n",
    "    #pmid,date1,date2,doi,label_category删除\n",
    "    train_data.append(( title, abstract, journal,keywords, pub_type, authors, label_id))\n",
    "\n",
    "for i in range(valid_df.shape[0]):\n",
    "    pmid, journal, title, abstract, keywords, label,pub_type, authors, date1, doi, date2 = valid_df.iloc[i, :]\n",
    "    label_id = [0] * len(labels)\n",
    "    for j, _ in enumerate(labels):\n",
    "        for separate_label in label.split(\";\"):\n",
    "            if _ == separate_label:\n",
    "                label_id[j] = 1\n",
    "    valid_data.append(( title, abstract,journal, keywords, pub_type, authors, label_id))\n",
    "for i in range(test_df.shape[0]):\n",
    "    pmid, journal, title, abstract, keywords, label,pub_type, authors, date1, doi, date2 = test_df.iloc[i, :]\n",
    "    label_id = [0] * len(labels)\n",
    "    for j, _ in enumerate(labels):\n",
    "        for separate_label in label.split(\";\"):\n",
    "            if _ == separate_label:\n",
    "                label_id[j] = 1\n",
    "    test_data.append(( title, abstract,journal, keywords, pub_type, authors, label_id))\n",
    "\n",
    "# print(train_data[:10])\n",
    "print(\"finish data processing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4510d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "\n",
    "    def __init__(self, data, batch_size=BATCH_SIZE):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = len(self.data) // self.batch_size\n",
    "        if len(self.data) % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            idxs = list(range(len(self.data)))\n",
    "            np.random.shuffle(idxs)\n",
    "            X1, X2, X3, X4,X5, X6,Y = [], [], [] ,[], [], [] ,[]\n",
    "            for i in idxs:\n",
    "                d = self.data[i]\n",
    "                #title\n",
    "                text1 = d[0][:maxlen]\n",
    "                #abstract\n",
    "                text2 = d[1][:maxlen]\n",
    "                #journal\n",
    "                text3 = d[2][:maxlen]\n",
    "                #keyword\n",
    "                text4 = d[3][:maxlen]\n",
    "                #pub_type\n",
    "                text5 = d[4][:maxlen]\n",
    "                #authors\n",
    "                text6 = d[5][:maxlen]\n",
    "                #文本数据将输入bert中，encode返回word_embedding和segmen_embedding(0是句子1,1是句子2\n",
    "                x1, x2 = tokenizer.encode(first=text1,second=text2)\n",
    "                x3, x4 = tokenizer.encode(first=text3,second=text4)\n",
    "                x5, x6 = tokenizer.encode(first=text5,second=text6)\n",
    "                y = d[6]\n",
    "                X1.append(x1)\n",
    "                X2.append(x2)\n",
    "                X3.append(x3)\n",
    "                X4.append(x4)\n",
    "                X5.append(x5)\n",
    "                X6.append(x6)\n",
    "                Y.append(y)\n",
    "                if len(X1) == self.batch_size or i == idxs[-1]:\n",
    "                    X1 = seq_padding(X1)\n",
    "                    X2 = seq_padding(X2)\n",
    "                    X3 = seq_padding(X3)\n",
    "                    X4 = seq_padding(X4)\n",
    "                    X5 = seq_padding(X5)\n",
    "                    X6 = seq_padding(X6)\n",
    "                    Y = seq_padding(Y)\n",
    "                    yield [X1, X2, X3, X4, X5, X6], Y\n",
    "                    [X1, X2, X3, X4,X5, X6, Y] = [], [], [], [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b4520e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cls_model(num_labels):\n",
    "    bert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, seq_len=None)\n",
    "\n",
    "    for layer in bert_model.layers:\n",
    "        layer.trainable = True\n",
    "\n",
    "    x1_in = Input(shape=(None,))\n",
    "    x2_in = Input(shape=(None,))\n",
    "    x3_in = Input(shape=(None,))\n",
    "    x4_in = Input(shape=(None,))\n",
    "    x5_in = Input(shape=(None,))\n",
    "    x6_in = Input(shape=(None,))\n",
    "    \n",
    "\n",
    "    bert_layer1 = bert_model([x1_in,x2_in])\n",
    "    bert_layer2 = bert_model([x3_in,x4_in])\n",
    "    bert_layer3 = bert_model([x5_in,x6_in])\n",
    "    cls_layer1 = Lambda(lambda x: x[:, 0])(bert_layer1)    # 取出[CLS]对应的向量用来做分类\n",
    "    cls_layer2 = Lambda(lambda x: x[:, 0])(bert_layer2)\n",
    "    cls_layer3 = Lambda(lambda x: x[:, 0])(bert_layer3)\n",
    "\n",
    "    x = Add()([cls_layer1,cls_layer2,cls_layer3])\n",
    "    x = Dropout(0.2)(x)\n",
    "    p = Dense(num_labels, activation='sigmoid')(x)     # 多分类\n",
    "\n",
    "    model = Model([x1_in,x2_in,x3_in,x4_in,x5_in,x6_in], p)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(1e-5), # 用足够小的学习率\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ade57db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_D = DataGenerator(train_data[:2600])\n",
    "#valid_D = DataGenerator(valid_data[:1000])\n",
    "train_D = DataGenerator(train_data)\n",
    "valid_D = DataGenerator(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f9d5799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 (None, None, 512)    53720064    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 512)          0           model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 512)          0           model_1[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 512)          0           model_1[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 512)          0           lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 8)            4104        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 53,724,168\n",
      "Trainable params: 53,724,168\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "begin model training...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "model = create_cls_model(len(labels))\n",
    "print(\"begin model training...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f296bdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "13105/13105 [==============================] - 4692s 358ms/step - loss: 0.2663 - accuracy: 0.6109 - val_loss: 0.2120 - val_accuracy: 0.7226\n",
      "Epoch 2/3\n",
      "13105/13105 [==============================] - 4694s 358ms/step - loss: 0.1944 - accuracy: 0.7215 - val_loss: 0.1814 - val_accuracy: 0.7616\n",
      "Epoch 3/3\n",
      "13105/13105 [==============================] - 4719s 360ms/step - loss: 0.1637 - accuracy: 0.7531 - val_loss: 0.1689 - val_accuracy: 0.7966\n",
      "finish model training!\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_D.__iter__(),\n",
    "        steps_per_epoch=len(train_D),\n",
    "        epochs=3,\n",
    "        validation_data=valid_D.__iter__(),\n",
    "        validation_steps=len(valid_D)\n",
    "    )\n",
    "\n",
    "print(\"finish model training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b05d154d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "WARNING:tensorflow:From <ipython-input-159-61b85cfcda19>:5: Model.evaluate_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.evaluate, which supports generators.\n",
      "模型评估结果: [0.1631319373846054, 0.7978361248970032]\n"
     ]
    }
   ],
   "source": [
    "model.save('multi-label-ee.h5')\n",
    "print(\"Model saved!\")\n",
    "\n",
    "test_D = DataGenerator(test_data)\n",
    "result = model.evaluate_generator(test_D.__iter__(), steps=len(test_D))\n",
    "print(\"模型评估结果:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a309685",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标题: Beating severe covid-19.\n",
      "作者: Wilson, Clare\n",
      "预测标签: ['General Info']\n",
      "实际标签: General Info\n",
      "cost time: 9.325701713562012\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras_bert import get_custom_objects\n",
    "import time\n",
    "# 加载训练好的模型\n",
    "model = load_model(\"multi-label-ee.h5\", custom_objects=get_custom_objects())\n",
    "tokenizer = OurTokenizer(token_dict)\n",
    "with open(\"label.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    label_dict = json.loads(f.read())\n",
    "\n",
    "maxlen = 250\n",
    "s_time = time.time()\n",
    "# 预测示例语句\n",
    "document = pd.read_csv('data/single text.csv').fillna(value=\"\")\n",
    "pmid, journal, title, abstract, keywords, label,pub_type, authors, date1, doi, date2, label_category = document.iloc[0, :]\n",
    "\n",
    "\n",
    "# 利用BERT进行tokenize\n",
    "journal = journal[:maxlen]\n",
    "title = title[:maxlen]\n",
    "abstract = abstract[:maxlen]\n",
    "keywords = keywords[:maxlen]\n",
    "pub_type = pub_type[:maxlen]\n",
    "authors = authors[:maxlen]\n",
    "x1, x2 = tokenizer.encode(first=title,second=abstract)\n",
    "x3, x4 = tokenizer.encode(first=journal,second=keywords)\n",
    "x5, x6 = tokenizer.encode(first=pub_type,second=authors)\n",
    "\n",
    "X1 = x1 + [0] * (maxlen-len(x1)) if len(x1) < maxlen else x1\n",
    "X2 = x2 + [0] * (maxlen-len(x2)) if len(x2) < maxlen else x2\n",
    "X3 = x3 + [0] * (maxlen-len(x3)) if len(x3) < maxlen else x3\n",
    "X4 = x4 + [0] * (maxlen-len(x4)) if len(x4) < maxlen else x4\n",
    "X5 = x5 + [0] * (maxlen-len(x5)) if len(x5) < maxlen else x5\n",
    "X6 = x6 + [0] * (maxlen-len(x6)) if len(x6) < maxlen else x6\n",
    "\n",
    "X1 = np.array(X1[:maxlen])\n",
    "X2 = np.array(X2[:maxlen])\n",
    "X3 = np.array(X3)\n",
    "X4 = np.array(X4)\n",
    "X5 = np.array(X5)\n",
    "X6 = np.array(X6)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 模型预测并输出预测结果\n",
    "prediction = model.predict([X1, X2, X3, X4, X5, X6])\n",
    "one_hot = np.where(prediction > 0.5, 1, 0)[0]\n",
    "\n",
    "\n",
    "print(\"标题: %s\" % title)\n",
    "print(\"作者: %s\" % authors)\n",
    "print(\"预测标签: %s\" % [label_dict[str(i)] for i in range(len(one_hot)) if one_hot[i]])\n",
    "print(\"实际标签: %s\" % label)\n",
    "e_time = time.time()\n",
    "print(\"cost time:\", e_time-s_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
